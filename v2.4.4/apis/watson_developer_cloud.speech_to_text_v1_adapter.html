
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>watson_developer_cloud.speech_to_text_v1_adapter module &#8212; Watson Developer Cloud Python SDK 2.4.4 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="watson_developer_cloud.text_to_speech_v1 module" href="watson_developer_cloud.text_to_speech_v1.html" />
    <link rel="prev" title="watson_developer_cloud.speech_to_text_v1 module" href="watson_developer_cloud.speech_to_text_v1.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-watson_developer_cloud.speech_to_text_v1_adapter">
<span id="watson-developer-cloud-speech-to-text-v1-adapter-module"></span><h1>watson_developer_cloud.speech_to_text_v1_adapter module<a class="headerlink" href="#module-watson_developer_cloud.speech_to_text_v1_adapter" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter">
<em class="property">class </em><code class="descname">SpeechToTextV1Adapter</code><span class="sig-paren">(</span><em>url='https://stream.watsonplatform.net/speech-to-text/api'</em>, <em>username=None</em>, <em>password=None</em>, <em>iam_apikey=None</em>, <em>iam_access_token=None</em>, <em>iam_url=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/watson_developer_cloud/speech_to_text_v1_adapter.html#SpeechToTextV1Adapter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="watson_developer_cloud.speech_to_text_v1.html#watson_developer_cloud.speech_to_text_v1.SpeechToTextV1" title="watson_developer_cloud.speech_to_text_v1.SpeechToTextV1"><code class="xref py py-class docutils literal notranslate"><span class="pre">watson_developer_cloud.speech_to_text_v1.SpeechToTextV1</span></code></a></p>
<dl class="method">
<dt id="watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter.recognize_using_websocket">
<code class="descname">recognize_using_websocket</code><span class="sig-paren">(</span><em>audio</em>, <em>content_type</em>, <em>recognize_callback</em>, <em>model=None</em>, <em>language_customization_id=None</em>, <em>acoustic_customization_id=None</em>, <em>customization_weight=None</em>, <em>base_model_version=None</em>, <em>inactivity_timeout=None</em>, <em>interim_results=None</em>, <em>keywords=None</em>, <em>keywords_threshold=None</em>, <em>max_alternatives=None</em>, <em>word_alternatives_threshold=None</em>, <em>word_confidence=None</em>, <em>timestamps=None</em>, <em>profanity_filter=None</em>, <em>smart_formatting=None</em>, <em>speaker_labels=None</em>, <em>http_proxy_host=None</em>, <em>http_proxy_port=None</em>, <em>customization_id=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/watson_developer_cloud/speech_to_text_v1_adapter.html#SpeechToTextV1Adapter.recognize_using_websocket"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter.recognize_using_websocket" title="Permalink to this definition">¶</a></dt>
<dd><p>Sends audio for speech recognition using web sockets.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>audio</strong> (<a class="reference internal" href="watson_developer_cloud.websocket.audio_source.html#watson_developer_cloud.websocket.audio_source.AudioSource" title="watson_developer_cloud.websocket.audio_source.AudioSource"><em>AudioSource</em></a>) – The audio to transcribe in the format specified by the</td>
</tr>
</tbody>
</table>
<p><cite>Content-Type</cite> header.
:param str content_type: The type of the input: audio/basic, audio/flac,
audio/l16, audio/mp3, audio/mpeg, audio/mulaw, audio/ogg, audio/ogg;codecs=opus,
audio/ogg;codecs=vorbis, audio/wav, audio/webm, audio/webm;codecs=opus, or
audio/webm;codecs=vorbis.
:param RecognizeCallback recognize_callback: The callback method for the websocket.
:param str model: The identifier of the model that is to be used for the
recognition request or, for the <strong>Create a session</strong> method, with the new session.
:param str language_customization_id: The customization ID (GUID) of a custom
language model that is to be used with the recognition request. The base model of
the specified custom language model must match the model specified with the
<cite>model</cite> parameter. You must make the request with service credentials created for
the instance of the service that owns the custom model. By default, no custom
language model is used. See [Custom
models](<a class="reference external" href="https://console.bluemix.net/docs/services/speech-to-text/input.html#custom">https://console.bluemix.net/docs/services/speech-to-text/input.html#custom</a>).
<strong>Note:</strong> Use this parameter instead of the deprecated <cite>customization_id</cite>
parameter.
:param str acoustic_customization_id: The customization ID (GUID) of a custom
acoustic model that is to be used with the recognition request or, for the
<strong>Create a session</strong> method, with the new session. The base model of the specified
custom acoustic model must match the model specified with the <cite>model</cite> parameter.
You must make the request with service credentials created for the instance of the
service that owns the custom model. By default, no custom acoustic model is used.
:param float customization_weight: If you specify the customization ID (GUID) of a
custom language model with the recognition request or, for sessions, with the
<strong>Create a session</strong> method, the customization weight tells the service how much
weight to give to words from the custom language model compared to those from the
base model for the current request.
Specify a value between 0.0 and 1.0. Unless a different customization weight was
specified for the custom model when it was trained, the default value is 0.3. A
customization weight that you specify overrides a weight that was specified when
the custom model was trained.
The default value yields the best performance in general. Assign a higher value if
your audio makes frequent use of OOV words from the custom model. Use caution when
setting the weight: a higher value can improve the accuracy of phrases from the
custom model’s domain, but it can negatively affect performance on non-domain
phrases.
:param str base_model_version: The version of the specified base model that is to
be used with recognition request or, for the <strong>Create a session</strong> method, with the
new session. Multiple versions of a base model can exist when a model is updated
for internal improvements. The parameter is intended primarily for use with custom
models that have been upgraded for a new base model. The default value depends on
whether the parameter is used with or without a custom model. For more
information, see [Base model
version](<a class="reference external" href="https://console.bluemix.net/docs/services/speech-to-text/input.html#version">https://console.bluemix.net/docs/services/speech-to-text/input.html#version</a>).
:param int inactivity_timeout: The time in seconds after which, if only silence
(no speech) is detected in submitted audio, the connection is closed with a 400
error. Useful for stopping audio submission from a live microphone when a user
simply walks away. Use <cite>-1</cite> for infinity.
:param list[str] keywords: An array of keyword strings to spot in the audio. Each
keyword string can include one or more tokens. Keywords are spotted only in the
final hypothesis, not in interim results. If you specify any keywords, you must
also specify a keywords threshold. You can spot a maximum of 1000 keywords. Omit
the parameter or specify an empty array if you do not need to spot keywords.
:param float keywords_threshold: A confidence value that is the lower bound for
spotting a keyword. A word is considered to match a keyword if its confidence is
greater than or equal to the threshold. Specify a probability between 0 and 1
inclusive. No keyword spotting is performed if you omit the parameter. If you
specify a threshold, you must also specify one or more keywords.
:param int max_alternatives: The maximum number of alternative transcripts to be
returned. By default, a single transcription is returned.
:param float word_alternatives_threshold: A confidence value that is the lower
bound for identifying a hypothesis as a possible word alternative (also known as
“Confusion Networks”). An alternative word is considered if its confidence is
greater than or equal to the threshold. Specify a probability between 0 and 1
inclusive. No alternative words are computed if you omit the parameter.
:param bool word_confidence: If <cite>true</cite>, a confidence measure in the range of 0 to
1 is returned for each word. By default, no word confidence measures are returned.
:param bool timestamps: If <cite>true</cite>, time alignment is returned for each word. By
default, no timestamps are returned.
:param bool profanity_filter: If <cite>true</cite> (the default), filters profanity from all
output except for keyword results by replacing inappropriate words with a series
of asterisks. Set the parameter to <cite>false</cite> to return results with no censoring.
Applies to US English transcription only.
:param bool smart_formatting: If <cite>true</cite>, converts dates, times, series of digits
and numbers, phone numbers, currency values, and internet addresses into more
readable, conventional representations in the final transcript of a recognition
request. For US English, also converts certain keyword strings to punctuation
symbols. By default, no smart formatting is performed. Applies to US English and
Spanish transcription only.
:param bool speaker_labels: If <cite>true</cite>, the response includes labels that identify
which words were spoken by which participants in a multi-person exchange. By
default, no speaker labels are returned. Setting <cite>speaker_labels</cite> to <cite>true</cite> forces
the <cite>timestamps</cite> parameter to be <cite>true</cite>, regardless of whether you specify <cite>false</cite>
for the parameter.
To determine whether a language model supports speaker labels, use the <strong>Get
models</strong> method and check that the attribute <cite>speaker_labels</cite> is set to <cite>true</cite>.
You can also refer to [Speaker
labels](<a class="reference external" href="https://console.bluemix.net/docs/services/speech-to-text/output.html#speaker_labels">https://console.bluemix.net/docs/services/speech-to-text/output.html#speaker_labels</a>).
:param str http_proxy_host: http proxy host name.
:param str http_proxy_port: http proxy port. If not set, set to 80.
:param str customization_id: <strong>Deprecated.</strong> Use the <cite>language_customization_id</cite>
parameter to specify the customization ID (GUID) of a custom language model that
is to be used with the recognition request. Do not specify both parameters with a
request.
:param dict headers: A <cite>dict</cite> containing the request headers
:return: A <cite>dict</cite> containing the <cite>SpeechRecognitionResults</cite> response.
:rtype: dict</p>
</dd></dl>

<dl class="method">
<dt id="watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter.add_corpus">
<code class="descname">add_corpus</code><span class="sig-paren">(</span><em>customization_id</em>, <em>corpus_name</em>, <em>corpus_file</em>, <em>allow_overwrite=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/watson_developer_cloud/speech_to_text_v1_adapter.html#SpeechToTextV1Adapter.add_corpus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#watson_developer_cloud.speech_to_text_v1_adapter.SpeechToTextV1Adapter.add_corpus" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a corpus.</p>
<p>Adds a single corpus text file of new training data to a custom language model.
Use multiple requests to submit multiple corpus text files. You must use
credentials for the instance of the service that owns a model to add a corpus to
it. Adding a corpus does not affect the custom language model until you train the
model for the new data by using the <strong>Train a custom language model</strong> method.
Submit a plain text file that contains sample sentences from the domain of
interest to enable the service to extract words in context. The more sentences you
add that represent the context in which speakers use words from the domain, the
better the service’s recognition accuracy. For guidelines about adding a corpus
text file and for information about how the service parses a corpus file, see
[Preparing a corpus text
file](<a class="reference external" href="https://console.bluemix.net/docs/services/speech-to-text/language-resource.html#prepareCorpus">https://console.bluemix.net/docs/services/speech-to-text/language-resource.html#prepareCorpus</a>).
The call returns an HTTP 201 response code if the corpus is valid. The service
then asynchronously processes the contents of the corpus and automatically
extracts new words that it finds. This can take on the order of a minute or two to
complete depending on the total number of words and the number of new words in the
corpus, as well as the current load on the service. You cannot submit requests to
add additional corpora or words to the custom model, or to train the model, until
the service’s analysis of the corpus for the current request completes. Use the
<strong>List a corpus</strong> method to check the status of the analysis.
The service auto-populates the model’s words resource with any word that is not
found in its base vocabulary; these are referred to as out-of-vocabulary (OOV)
words. You can use the <strong>List custom words</strong> method to examine the words resource,
using other words method to eliminate typos and modify how words are pronounced as
needed.
To add a corpus file that has the same name as an existing corpus, set the
<cite>allow_overwrite</cite> parameter to <cite>true</cite>; otherwise, the request fails. Overwriting
an existing corpus causes the service to process the corpus text file and extract
OOV words anew. Before doing so, it removes any OOV words associated with the
existing corpus from the model’s words resource unless they were also added by
another corpus or they have been modified in some way with the <strong>Add custom
words</strong> or <strong>Add a custom word</strong> method.
The service limits the overall amount of data that you can add to a custom model
to a maximum of 10 million total words from all corpora combined. Also, you can
add no more than 30 thousand custom (OOV) words to a model; this includes words
that the service extracts from corpora and words that you add directly.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>customization_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – The customization ID (GUID) of the custom language</td>
</tr>
</tbody>
</table>
<p>model. You must make the request with service credentials created for the instance
of the service that owns the custom model.
:param str corpus_name: The name of the corpus for the custom language model. When
adding a corpus, do not include spaces in the name; use a localized name that
matches the language of the custom model; and do not use the name <cite>user</cite>, which is
reserved by the service to denote custom words added or modified by the user.
:param file corpus_file: A plain text file that contains the training data for the
corpus. Encode the file in UTF-8 if it contains non-ASCII characters; the service
assumes UTF-8 encoding if it encounters non-ASCII characters. With cURL, use the
<cite>–data-binary</cite> option to upload the file for the request.
:param bool allow_overwrite: If <cite>true</cite>, the specified corpus or audio resource
overwrites an existing corpus or audio resource with the same name. If <cite>false</cite>,
the request fails if a corpus or audio resource with the same name already exists.
The parameter has no effect if a corpus or audio resource with the same name does
not already exist.
:param dict headers: A <cite>dict</cite> containing the request headers
:return: A <cite>DetailedResponse</cite> containing the result, headers and HTTP status code.
:rtype: DetailedResponse</p>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Useful Links</h3>
<ul>
  <li><a href="https://www.ibm.com/watson/developercloud/">Watson APIs</a></li>
  <li><a href="http://github.com/watson-developer-cloud/python-sdk">GitHub</a></li>
  <li><a href="http://github.com/watson-developer-cloud/python-sdk/issues">Issue Tracker</a></li>
  <li><a href="http://stackoverflow.com/questions/ask?tags=ibm-watson">StackOverflow</a></li>
</ul><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="watson_developer_cloud.speech_to_text_v1.html" title="previous chapter">watson_developer_cloud.speech_to_text_v1 module</a></li>
      <li>Next: <a href="watson_developer_cloud.text_to_speech_v1.html" title="next chapter">watson_developer_cloud.text_to_speech_v1 module</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      
      
    </div>

    

    
  </body>
</html>